# This file is versioned.
# Please copy and rename this config file to a private config.yaml

sim:  # simulations
  env:
    size: 20  # Size of the board
    number_coins: 60  # Number of coins on the board
    max_length: 100  # Maximum number of iterations before terminal state
    state:
      type: progressive  # in [progressive, hard]
      depth_of_field: 5  # only if progressive state

  output:
    path: path/to/output/videos
    save_figs: Yes
    save_every: 100  # Save results every 100 episodes

  agent:
    type: curious  # in ["dqn", "default", "curious"]

learning:
  cuda: Yes
  save_models: Yes
  load_model: No
  batch_size: 64
  num_episodes: 1000 # Number of episodes of learning

  gamma: 0.999 # Discount factor.

  eps_start: 0.8
  eps_end: 0.1
  eps_decay: 1000

  lr_actor: 0.01 # Learning rate for the actor, or dqn
  lr_critic: 0.01 # Learning rate
  update_frequency: 100 # Update the target net every...
  tau: 0.5

  gumbel_softmax:
    use: Yes
    tau: 0.5

  icm:
    eta: 0.2
    beta: 0.2
    lbd: 0.1
    features:
      dim: 128
      lr: 0.001
    forward_model:
      lr: 0.001
    inverse_model:
      lr: 0.001

testing:
  policy:
    random_action_prob: 0.1

experience_replay:
  size: 10000  # Size of the experience replay buffer

metrics:
  train_cycle_length: 100 # in number of episodes (metrics are averaged over)
  test_cycle_length: 100
